# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Крысин Никита Александрович
- РИ210940

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Ознакомиться с основными операторами зыка Python на примере реализации линейной регрессии.

## Задание 1
### Измените параметры файла. yaml-агента и определить какие параметры и
как влияют на обучение модели.
![Снимок экрана (116)](https://user-images.githubusercontent.com/114180894/204645166-5a70c1b8-b23c-41da-9c58-39977c1c5058.png)

## Задание 2
### Опишите результаты, выведенные в TensorBoard.
![Снимок экрана (115)](https://user-images.githubusercontent.com/114180894/204645367-1689e301-dc30-40bf-8073-ecce3f2ef8da.png)
![Снимок экрана (117)](https://user-images.githubusercontent.com/114180894/204645346-28a7fcca-2916-46b2-a39b-39c8cc66ca03.png)
Environment Cumulative Reward - средняя совокупность по всем агентам. У нас оно увеличивается во время успешных итераций и снижается от обратных.
Environment Episode Length - средняя продолжительность каждой итерации для всех агентов. В нашем случае растет в случае увеличения времени и наоборот.
Policy Loss - средняя величина функции потерь. Величина должна уменьшаться во время успешной итерации 
Value Loss  -средняя потеря значений при обновлении функции. Этот показатель должен увеличиваться пока агент учится, а после стабилизации значения вознаграждения, уменьшаться.
## Выводы


| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
